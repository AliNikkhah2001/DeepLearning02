{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7326079,"sourceType":"datasetVersion","datasetId":4252200},{"sourceId":7379212,"sourceType":"datasetVersion","datasetId":4288280}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 style=\"text-align: center\">\nDeep Learning HW4 </br>\nPoem generation,fine tuning\n</h1>\n","metadata":{}},{"cell_type":"markdown","source":"\n  <style>\n    table {\n      width: 100%;\n      border-collapse: collapse;\n    }\n\n    th, td {\n      border: 1px solid #dddddd;\n      text-align: left;\n      padding: 8px;\n    }\n\n    th {\n      background-color: #f2f2f2;\n    }\n\n    .box {\n      border: 1px solid #000;\n      padding: 10px;\n      width: 400px; /* Adjust the width as needed */\n      margin: 20px auto;\n    }\n  </style>\n</head>\n<body>\n\n<div class=\"box\">\n  <table>\n    <tr>\n      <th colspan=\"2\">Personal Info</th>\n    </tr>\n    <tr>\n      <td>First Name:</td>\n      <td>Ali</td>\n    </tr>\n    <tr>\n      <td>Last Name:</td>\n      <td>Nikkhah</td>\n    </tr>\n    <tr>\n      <td>Student Number:</td>\n      <td>99102445</td>\n    </tr>\n    <tr>\n      <td>Git:</td>\n      <td><a href=\"https://github.com/AliNikkhah2001/DataScience02\" target=\"_blank\">https://github.com/AliNikkhah2001/DeepLearning02</a></td>\n    </tr>\n  </table>\n</div>\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-06T07:00:51.995698Z","iopub.execute_input":"2024-01-06T07:00:51.996217Z","iopub.status.idle":"2024-01-06T07:00:52.047189Z","shell.execute_reply.started":"2024-01-06T07:00:51.996165Z","shell.execute_reply":"2024-01-06T07:00:52.045966Z"}}},{"cell_type":"markdown","source":"# Poem generator using Ferdowsi poems dataset","metadata":{}},{"cell_type":"markdown","source":"## Data Import and initialization","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install keras_preprocessing\n!pip install transformers\n","metadata":{"execution":{"iopub.status.busy":"2024-01-11T09:44:27.013670Z","iopub.execute_input":"2024-01-11T09:44:27.014036Z","iopub.status.idle":"2024-01-11T09:44:50.362843Z","shell.execute_reply.started":"2024-01-11T09:44:27.014005Z","shell.execute_reply":"2024-01-11T09:44:50.361607Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"%%capture\nimport tensorflow as tf\nimport numpy as np\nimport math\nimport os\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, LSTM, GRU, Embedding\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\nfrom transformers import TextDataset, DataCollatorForLanguageModeling\nfrom transformers import Trainer, TrainingArguments\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n","metadata":{"execution":{"iopub.status.busy":"2024-01-11T09:44:50.364995Z","iopub.execute_input":"2024-01-11T09:44:50.365320Z","iopub.status.idle":"2024-01-11T09:44:57.084334Z","shell.execute_reply.started":"2024-01-11T09:44:50.365291Z","shell.execute_reply":"2024-01-11T09:44:57.083549Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"\nfrom pathlib import Path\nimport torch\nfrom transformers import AutoTokenizer, AutoConfig, AutoModelWithLMHead\nfrom transformers import AutoTokenizer, GPT2LMHeadModel, GPT2Config\nfrom IPython import display\n     ","metadata":{"execution":{"iopub.status.busy":"2024-01-11T09:44:57.085463Z","iopub.execute_input":"2024-01-11T09:44:57.086162Z","iopub.status.idle":"2024-01-11T09:44:57.094541Z","shell.execute_reply.started":"2024-01-11T09:44:57.086126Z","shell.execute_reply":"2024-01-11T09:44:57.093517Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Open text file and make dataframe ","metadata":{}},{"cell_type":"code","source":"!git lfs install\n!git clone https://huggingface.co/HooshvareLab/gpt2-fa-poetry","metadata":{"execution":{"iopub.status.busy":"2024-01-11T09:44:57.096791Z","iopub.execute_input":"2024-01-11T09:44:57.097098Z","iopub.status.idle":"2024-01-11T09:44:59.049990Z","shell.execute_reply.started":"2024-01-11T09:44:57.097072Z","shell.execute_reply":"2024-01-11T09:44:59.049027Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Git LFS initialized.\nfatal: destination path 'gpt2-fa-poetry' already exists and is not an empty directory.\n","output_type":"stream"}]},{"cell_type":"code","source":"data_path='/kaggle/input/ferdowsi/ferdousi.txt'","metadata":{"execution":{"iopub.status.busy":"2024-01-11T09:44:59.051302Z","iopub.execute_input":"2024-01-11T09:44:59.051607Z","iopub.status.idle":"2024-01-11T09:44:59.056548Z","shell.execute_reply.started":"2024-01-11T09:44:59.051577Z","shell.execute_reply":"2024-01-11T09:44:59.055565Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"with open(data_path, 'r', encoding='utf-8') as f:\n    lines = f.read().split('\\n')\nlines = lines[2:]\n","metadata":{"execution":{"iopub.status.busy":"2024-01-11T09:44:59.057953Z","iopub.execute_input":"2024-01-11T09:44:59.058274Z","iopub.status.idle":"2024-01-11T09:44:59.110768Z","shell.execute_reply.started":"2024-01-11T09:44:59.058250Z","shell.execute_reply":"2024-01-11T09:44:59.110102Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"model_name_or_path = \"HooshvareLab/gpt2-fa\"\n\ntokenizer = AutoTokenizer.from_pretrained(\n    model_name_or_path,\n    bos_token='<s>', \n    eos_token='</s>', \n    pad_token='<pad>',\n    unk_token='<unk>'\n)\ntokenizer.add_special_tokens({\n    \"bos_token\": '</s>',\n    \"eos_token\": '</s>', \n    \"pad_token\": '<pad>',\n    \"unk_token\": '<unk>'\n})\n\nconfig = AutoConfig.from_pretrained(\n    model_name_or_path,\n    bos_token_id=tokenizer(\"<s>\")[\"input_ids\"][0], \n    eos_token_id=tokenizer(\"</s>\")[\"input_ids\"][0], \n    pad_token_id=tokenizer(\"<pad>\")[\"input_ids\"][0],\n    unk_token_id=tokenizer(\"<unk>\")[\"input_ids\"][0],\n)\n\ntokenizer.save_pretrained(\"/kaggle/working/gpt2-fa/\")\nconfig.save_pretrained(\"/kaggle/working/gpt2-fa/\")","metadata":{"execution":{"iopub.status.busy":"2024-01-11T09:44:59.111905Z","iopub.execute_input":"2024-01-11T09:44:59.112539Z","iopub.status.idle":"2024-01-11T09:44:59.520632Z","shell.execute_reply.started":"2024-01-11T09:44:59.112505Z","shell.execute_reply":"2024-01-11T09:44:59.519859Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"df_input=[]\ndf_target=[]\ndf_concat=[]\nfor i in range(0, len(lines)-3, 2):\n    input_text= lines[i] \n    target_text = ' <s> '+ lines[i+1]+'   ' +lines[i+2] + '    ' + lines[i+3]+'  </s>  '\n    df_input.append(input_text)\n    df_target.append(target_text)\n    df_concat.append(input_text+target_text)","metadata":{"execution":{"iopub.status.busy":"2024-01-11T09:44:59.521746Z","iopub.execute_input":"2024-01-11T09:44:59.522015Z","iopub.status.idle":"2024-01-11T09:44:59.621000Z","shell.execute_reply.started":"2024-01-11T09:44:59.521991Z","shell.execute_reply":"2024-01-11T09:44:59.620085Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"print(len(df_input))\nprint(len(df_target))","metadata":{"execution":{"iopub.status.busy":"2024-01-11T09:44:59.622052Z","iopub.execute_input":"2024-01-11T09:44:59.622350Z","iopub.status.idle":"2024-01-11T09:44:59.627544Z","shell.execute_reply.started":"2024-01-11T09:44:59.622325Z","shell.execute_reply":"2024-01-11T09:44:59.626541Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"49607\n49607\n","output_type":"stream"}]},{"cell_type":"code","source":"idx = 50\nprint(f'{df_input[idx]}')\nprint(f'{df_target[idx]}')\nprint(f'{df_concat[idx]}')","metadata":{"execution":{"iopub.status.busy":"2024-01-11T09:44:59.631513Z","iopub.execute_input":"2024-01-11T09:44:59.631772Z","iopub.status.idle":"2024-01-11T09:44:59.638017Z","shell.execute_reply.started":"2024-01-11T09:44:59.631749Z","shell.execute_reply":"2024-01-11T09:44:59.637109Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"همی بر شد آتش فرود آمد آب\n <s> همی گشت گرد زمین آفتاب   گیا رست با چند گونه درخت    به زیر اندر آمد سرانشان ز بخت  </s>  \nهمی بر شد آتش فرود آمد آب <s> همی گشت گرد زمین آفتاب   گیا رست با چند گونه درخت    به زیر اندر آمد سرانشان ز بخت  </s>  \n","output_type":"stream"}]},{"cell_type":"code","source":"\nprint(tokenizer.encode(\"<s>گیا رست با چند گونه درخت    به زیر اندر آمد سرانشان ز بخت</s>\"))\nprint(tokenizer.encode(\"<s>\"))\nprint(tokenizer.encode(\"</s>\"))\nprint(tokenizer.encode(\"<pad>\"))\nprint(tokenizer.encode(\"<|startoftext|>\"))\nprint(tokenizer.encode(\"<sep>\"))","metadata":{"execution":{"iopub.status.busy":"2024-01-11T09:44:59.639178Z","iopub.execute_input":"2024-01-11T09:44:59.639460Z","iopub.status.idle":"2024-01-11T09:44:59.647930Z","shell.execute_reply.started":"2024-01-11T09:44:59.639436Z","shell.execute_reply":"2024-01-11T09:44:59.647084Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"[0, 423, 268, 2630, 314, 638, 1752, 3087, 21147, 303, 730, 15233, 2199, 3325, 549, 355, 11653, 2]\n[0]\n[2]\n[1]\n[6]\n[9]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## MultiGPU setup","metadata":{}},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2024-01-11T09:44:59.648896Z","iopub.execute_input":"2024-01-11T09:44:59.649160Z","iopub.status.idle":"2024-01-11T09:45:00.719339Z","shell.execute_reply.started":"2024-01-11T09:44:59.649130Z","shell.execute_reply":"2024-01-11T09:45:00.718401Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Thu Jan 11 09:45:00 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n| N/A   64C    P8              11W /  70W |      0MiB / 15360MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n|   1  Tesla T4                       Off | 00000000:00:05.0 Off |                    0 |\n| N/A   64C    P8              11W /  70W |      0MiB / 15360MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|  No running processes found                                                           |\n+---------------------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\n\ndef set_up_two_gpus():\n    available_gpus = torch.cuda.device_count()\n    if available_gpus < 2:\n        raise ValueError(\"Insufficient number of GPUs available. Need at least two GPUs.\")\n\n    device_1 = torch.device(\"cuda:0\")\n    device_2 = torch.device(\"cuda:1\")\n\n    print(f\"Available GPUs: {available_gpus}\")\n    print(f\"Setting GPU 0 as device: {device_1}\")\n    print(f\"Setting GPU 1 as device: {device_2}\")\n\n    return device_1, device_2\n\n# Check and print if the GPUs are working by performing a computational operation\ndef check_gpu(device):\n    tensor = torch.randn(1000, 1000, device=device)\n    result = torch.matmul(tensor, tensor)\n    return result\n\n# Usage\ndevice_1, device_2 = set_up_two_gpus()\nresult_1 = check_gpu(device_1)\nresult_2 = check_gpu(device_2)\nprint(\"Results from GPU 0:\", result_1)\nprint(\"Results from GPU 1:\", result_2)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-11T09:45:00.720785Z","iopub.execute_input":"2024-01-11T09:45:00.721116Z","iopub.status.idle":"2024-01-11T09:45:01.185866Z","shell.execute_reply.started":"2024-01-11T09:45:00.721085Z","shell.execute_reply":"2024-01-11T09:45:01.184888Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Available GPUs: 2\nSetting GPU 0 as device: cuda:0\nSetting GPU 1 as device: cuda:1\nResults from GPU 0: tensor([[ 15.7754,  28.1131,  19.3813,  ...,   2.1333, -21.8276, -18.7964],\n        [ 21.8896, -23.0110,  42.5994,  ...,  -2.9641,  12.9708, -14.1444],\n        [-21.1606,   1.9071,  -1.7755,  ...,  21.4530, -31.3334,   6.3431],\n        ...,\n        [-34.3632,  -2.3821,  -7.7075,  ...,  11.2810, -90.0004, -10.4166],\n        [-32.6388, -29.5234, -15.5900,  ..., -30.7012,  -5.7184, -68.0117],\n        [ 28.7030, -25.0085, -13.7762,  ..., -22.9083,  28.3056, -52.3145]],\n       device='cuda:0')\nResults from GPU 1: tensor([[-48.7228, -59.0386, -28.8195,  ...,  22.8311,  24.0622,  35.2678],\n        [ 29.6720,  12.5468, -67.6738,  ...,   4.0252,  48.0882,  80.3356],\n        [-20.7171, -42.0483,  69.3719,  ...,  38.2693,  45.1786,  42.2385],\n        ...,\n        [ 13.9805, -24.3852,  43.2172,  ...,   9.4317,  -4.9429, -48.4698],\n        [ 11.4831,  -0.7422,   2.8613,  ..., -27.0245,  14.7413, -56.6707],\n        [ 20.7436,  -2.8421, -40.9040,  ..., -15.1738,  51.3475,  19.7048]],\n       device='cuda:1')\n","output_type":"stream"}]},{"cell_type":"code","source":"# Sample input text\ninput_text = df_input[50]\n# Tokenize and encode the input text\ninput_encoding = tokenizer.encode(input_text)\n\n# Decode the tokenized input back to text\ndecoded_input = tokenizer.decode(input_encoding)\n\n# Print results\nprint(\"Original Text:\", input_text)\nprint(\"Tokenized Input:\", input_encoding)\nprint(\"Decoded Input:\", decoded_input)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-11T09:45:01.187278Z","iopub.execute_input":"2024-01-11T09:45:01.187652Z","iopub.status.idle":"2024-01-11T09:45:01.195765Z","shell.execute_reply.started":"2024-01-11T09:45:01.187617Z","shell.execute_reply":"2024-01-11T09:45:01.194932Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Original Text: همی بر شد آتش فرود آمد آب\nTokenized Input: [272, 398, 327, 403, 2466, 2638, 2199, 797]\nDecoded Input: همی بر شد آتش فرود آمد آب\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.utils.data import Dataset  \nimport torch\ntorch.manual_seed(42)\nclass MTGDataset(Dataset):\n    def __init__(self, input_texts, tokenizer, max_length=1024):\n        self.tokenizer = tokenizer  \n        self.input_ids = []\n        self.attn_masks = []\n\n        for txt in input_texts:\n            encodings_dict = tokenizer(txt ,\n                                       truncation=True,\n                                       max_length=max_length,\n                                       padding=\"max_length\")\n            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        return self.input_ids[idx], self.attn_masks[idx]","metadata":{"execution":{"iopub.status.busy":"2024-01-11T09:45:01.196810Z","iopub.execute_input":"2024-01-11T09:45:01.197096Z","iopub.status.idle":"2024-01-11T09:45:01.209168Z","shell.execute_reply.started":"2024-01-11T09:45:01.197072Z","shell.execute_reply":"2024-01-11T09:45:01.208248Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import random_split\nmax_seq = 256\ndataset = MTGDataset(df_concat, tokenizer, max_length=max_seq)\n# Split into training and validation sets\ntrain_size = int(0.9 * len(dataset))\nval_size = (len(dataset) - train_size)\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\nf'There are {len(train_dataset)} samples for training, and {len(val_dataset)} samples for validation testing'","metadata":{"execution":{"iopub.status.busy":"2024-01-11T09:45:01.210523Z","iopub.execute_input":"2024-01-11T09:45:01.210821Z","iopub.status.idle":"2024-01-11T09:45:23.206984Z","shell.execute_reply.started":"2024-01-11T09:45:01.210797Z","shell.execute_reply":"2024-01-11T09:45:23.205982Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"'There are 44646 samples for training, and 4961 samples for validation testing'"},"metadata":{}}]},{"cell_type":"code","source":"import random\nfrom transformers import GPT2LMHeadModel, GPT2Config\nimport numpy as np\n\n# Loading the model configuration and setting it to the GPT2 standard settings.\nconfiguration = GPT2Config.from_pretrained('/kaggle/working/gpt2-fa/', output_hidden_states=False)\n\n# Create the instance of the model and set the token size embedding length\nmodel = GPT2LMHeadModel.from_pretrained(\"/kaggle/working/gpt2-fa/\", config=configuration)\nmodel.resize_token_embeddings(len(tokenizer))\n\n\n# This step is optional but will enable reproducible runs.\nseed_val = 42\n\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)","metadata":{"execution":{"iopub.status.busy":"2024-01-11T09:45:23.208245Z","iopub.execute_input":"2024-01-11T09:45:23.208527Z","iopub.status.idle":"2024-01-11T09:45:23.801461Z","shell.execute_reply.started":"2024-01-11T09:45:23.208502Z","shell.execute_reply":"2024-01-11T09:45:23.800668Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"epochs = 3\nwarmup_steps = 1e2\nsample_every = 300","metadata":{"execution":{"iopub.status.busy":"2024-01-11T09:45:23.802794Z","iopub.execute_input":"2024-01-11T09:45:23.803086Z","iopub.status.idle":"2024-01-11T09:45:23.807310Z","shell.execute_reply.started":"2024-01-11T09:45:23.803061Z","shell.execute_reply":"2024-01-11T09:45:23.806318Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"from transformers import AdamW\n\n# AdamW is a class from the huggingface library, it is the optimizer we will be using, and we will only be instantiating it with the default parameters.\noptimizer = AdamW(\n    model.parameters(),\n    lr=5e-4,\n    eps=1e-8,\n    no_deprecation_warning=True\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-11T09:45:23.808610Z","iopub.execute_input":"2024-01-11T09:45:23.808892Z","iopub.status.idle":"2024-01-11T09:45:23.821045Z","shell.execute_reply.started":"2024-01-11T09:45:23.808867Z","shell.execute_reply":"2024-01-11T09:45:23.820085Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n\ntrain_dataloader = DataLoader(\n    train_dataset,\n    sampler=RandomSampler(train_dataset),\n    batch_size=8\n)\n\nvalidation_dataloader = DataLoader(\n    val_dataset,\n    sampler=SequentialSampler(val_dataset),\n    batch_size=8\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-11T09:45:23.822451Z","iopub.execute_input":"2024-01-11T09:45:23.822764Z","iopub.status.idle":"2024-01-11T09:45:23.834114Z","shell.execute_reply.started":"2024-01-11T09:45:23.822741Z","shell.execute_reply":"2024-01-11T09:45:23.833161Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"from transformers import get_linear_schedule_with_warmup\ntotal_steps = len(train_dataloader) * epochs\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=warmup_steps,\n    num_training_steps=total_steps)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-11T09:45:23.835186Z","iopub.execute_input":"2024-01-11T09:45:23.835489Z","iopub.status.idle":"2024-01-11T09:45:23.844241Z","shell.execute_reply.started":"2024-01-11T09:45:23.835465Z","shell.execute_reply":"2024-01-11T09:45:23.843374Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"from torch.nn import DataParallel\n# Define a function to set up multi-GPU training\ndef multi_gpu_setup(model):\n    if torch.cuda.device_count() > 1:\n        print(f\"Using {torch.cuda.device_count()} GPUs\")\n        model = DataParallel(model)\n    return model\n\n# Use the function to set up multi-GPU training\nmodel = multi_gpu_setup(model)","metadata":{"execution":{"iopub.status.busy":"2024-01-11T09:45:23.845622Z","iopub.execute_input":"2024-01-11T09:45:23.845886Z","iopub.status.idle":"2024-01-11T09:45:23.856267Z","shell.execute_reply.started":"2024-01-11T09:45:23.845862Z","shell.execute_reply":"2024-01-11T09:45:23.855344Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Using 2 GPUs\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nif device.type == 'cuda':\n    print(f\"Memory Allocated: {torch.cuda.memory_allocated(device)/1024**3:.2f} GB\")\n    print(f\"Memory Cached: {torch.cuda.memory_cached(device)/1024**3:.2f} GB\")\nelse:\n    print(\"CUDA is not available. Switching to CPU.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-01-11T09:45:23.857379Z","iopub.execute_input":"2024-01-11T09:45:23.857677Z","iopub.status.idle":"2024-01-11T09:45:23.868744Z","shell.execute_reply.started":"2024-01-11T09:45:23.857653Z","shell.execute_reply":"2024-01-11T09:45:23.867793Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Using device: cuda\nMemory Allocated: 0.01 GB\nMemory Cached: 0.02 GB\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:416: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"\nsample_input = df_input[np.random.randint(0, len(df_input))]\nprint(sample_input)\nsample_input_ids = torch.tensor(tokenizer([sample_input])[\"input_ids\"])\nsample_input_ids = sample_input_ids.to('cpu')\n\nsample_outputs = model.module.generate(\n    input_ids=sample_input_ids,\n    do_sample=True,\n    top_k=50,\n    max_length=50,\n    top_p=0.95,\n    num_return_sequences=5\n)\nfor i, sample_output in enumerate(sample_outputs):\n    gen_sample_output = tokenizer.decode(sample_output, skip_special_tokens=False)\n    gen_sample_output = gen_sample_output.replace(\"<|startoftext|>\", \"\\n\")\n    gen_sample_output = gen_sample_output.replace(\"<s>\", \"\")\n    gen_sample_output = gen_sample_output.replace(\"</s>\", \"\")\n    gen_sample_output = gen_sample_output.replace(\"<sep>\", \"\\n\")\n\n    print(f'Example output: {gen_sample_output}')","metadata":{"execution":{"iopub.status.busy":"2024-01-11T09:45:23.869950Z","iopub.execute_input":"2024-01-11T09:45:23.870259Z","iopub.status.idle":"2024-01-11T09:45:27.374166Z","shell.execute_reply.started":"2024-01-11T09:45:23.870228Z","shell.execute_reply":"2024-01-11T09:45:27.373158Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"جوان گرچه دانا بود با گهر\nExample output: جوان گرچه دانا بود با گهربار و با فراستش این کار را کرد و مردم را برانداخت به تباهی. پس از او، نوبت به یزدگرد رسید. سپاه به سوی غرب رفت. سپاه عرب در آن سوی جیحون (که در زبان فارسی از\nExample output: جوان گرچه دانا بود با گهربار عشق» اما در این میان که چه چیزی از قلم افتاده است. در این میان می‌بینم «ای مردم، چرا نباید در دل آتش به گریه بیفتد؟» تا این که یک نفر از مردم «هست\nExample output: جوان گرچه دانا بود با گهربار که این هم یک بار خیلی هم آدم را خسته نکرد و از فرط خستگی به خودش آمد. او را با این حال صدا می‌کند، می‌گوید: «با همین حال می‌توانم به خودم امید\nExample output: جوان گرچه دانا بود با گهربار در کوچه پس‌کوچه‌ها قدم زد. با صدای آواز دلکش و نغمه‌هایی دلنشین از بنان. به آواز روح می‌افروزم و آرام می‌نشیند. می‌آید و می‌نشیند\nExample output: جوان گرچه دانا بود با گهربار و با خود می‌گوید من هنوز به اندازهٔ تو برای زندگی کردن احتیاج دارم (پس من هم به همین نسبت احتیاج دارم) و فقط وقتی بزرگ شدم این مشکل‌ها را برطرف و از آن‌ها\n","output_type":"stream"}]},{"cell_type":"code","source":"%%capture\n!pip install tqdm","metadata":{"execution":{"iopub.status.busy":"2024-01-11T09:45:27.375782Z","iopub.execute_input":"2024-01-11T09:45:27.376113Z","iopub.status.idle":"2024-01-11T09:45:38.911933Z","shell.execute_reply.started":"2024-01-11T09:45:27.376084Z","shell.execute_reply":"2024-01-11T09:45:38.910881Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-01-11T09:45:38.913519Z","iopub.execute_input":"2024-01-11T09:45:38.913829Z","iopub.status.idle":"2024-01-11T09:45:38.919535Z","shell.execute_reply.started":"2024-01-11T09:45:38.913800Z","shell.execute_reply":"2024-01-11T09:45:38.918640Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\n","metadata":{"execution":{"iopub.status.busy":"2024-01-11T09:46:27.021044Z","iopub.execute_input":"2024-01-11T09:46:27.021450Z","iopub.status.idle":"2024-01-11T09:46:27.025938Z","shell.execute_reply.started":"2024-01-11T09:46:27.021417Z","shell.execute_reply":"2024-01-11T09:46:27.024887Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"\ntraining_args = TrainingArguments(\n    output_dir=\"./output\",\n    num_train_epochs=1,\n    per_device_train_batch_size=32,\n    learning_rate=5e-5,\n    logging_steps=500,\n    save_steps=1000,\n    evaluation_strategy=\"steps\",\n    eval_steps=1000,\n    # Add distributed training options\n    per_device_eval_batch_size=32,\n    fp16=True,  # Enable mixed-precision training if supported\n    dataloader_num_workers=16,  # Adjust based on your system capabilities\n    report_to=\"tensorboard\",  # You can use TensorBoard for logging\n)\n\n# Initialize Trainer object\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    tokenizer=tokenizer,\n)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-11T09:45:38.936924Z","iopub.execute_input":"2024-01-11T09:45:38.937259Z","iopub.status.idle":"2024-01-11T09:45:39.110405Z","shell.execute_reply.started":"2024-01-11T09:45:38.937228Z","shell.execute_reply":"2024-01-11T09:45:39.109666Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"import random\nimport time\nimport datetime\ndef format_time(elapsed):\n    return str(datetime.timedelta(seconds=int(round((elapsed)))))\ntotal_t0 = time.time()\ntraining_stats = []\nmodel = model.to(device)\nfor epoch_i in tqdm(range(training_args.num_train_epochs),position=0):\n    print(f'Beginning epoch {epoch_i + 1} of {training_args.num_train_epochs}')\n    t0 = time.time()\n    total_train_loss = 0\n    model.train()\n    for step, batch in tqdm(enumerate(train_dataloader), total=len(train_dataloader), position=0):\n        b_input_ids = batch[0].to(device)\n        b_labels = batch[0].to(device)\n        b_masks = batch[1].to(device)\n        model.zero_grad()\n        outputs = model(b_input_ids, labels=b_labels, attention_mask=b_masks, token_type_ids=None)\n        loss = outputs.loss\n        loss = loss.mean()\n        batch_loss = loss\n        total_train_loss += batch_loss\n        # Get sample every 100 batches.\n        if step % sample_every == 0 and not step == 0:\n            elapsed = format_time(time.time() - t0)\n            print()\n            print(f'Batch {step} of {len(train_dataloader)}. Loss:{batch_loss}. Time:{elapsed}')\n            model.eval()\n            sample_input = df_input[np.random.randint(0, len(df_input))]\n            print(sample_input)\n            sample_input_ids = torch.tensor(tokenizer([sample_input])[\"input_ids\"])\n            sample_input_ids = sample_input_ids.to(device)\n            sample_outputs = model.module.generate(\n                input_ids=sample_input_ids,\n                do_sample=True,\n                top_k=50,\n                max_length=50,\n                top_p=0.95,\n                num_return_sequences=1\n            )\n            for i, sample_output in enumerate(sample_outputs):\n                gen_sample_output = tokenizer.decode(sample_output, skip_special_tokens=False)\n                gen_sample_output = gen_sample_output.replace(\"<|startoftext|>\", \"\\n\")\n                gen_sample_output = gen_sample_output.replace(\"<s>\", \"\")\n                gen_sample_output = gen_sample_output.replace(\"</s>\", \"\")\n                gen_sample_output = gen_sample_output.replace(\"<sep>\", \"\\n\")\n                print(f'Example output: {gen_sample_output}')\n            model.train()\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n    # Calculate the average loss over all of the batches.\n    avg_train_loss = total_train_loss / len(train_dataloader)\n    # Measure how long this epoch took.\n    training_time = format_time(time.time() - t0)\n    print()\n    print(f'Average Training Loss: {avg_train_loss}. Epoch time: {training_time}')\n    print()\n    t0 = time.time()\n    model.eval()\n    total_eval_loss = 0\n    nb_eval_steps = 0\n    # Evaluate data for one epoch\n    for batch in tqdm(validation_dataloader, total=len(validation_dataloader), position=0):\n        b_input_ids = batch[0].to(device)\n        b_labels = batch[0].to(device)\n        b_masks = batch[1].to(device)\n        with torch.no_grad():\n            outputs = model(b_input_ids, attention_mask=b_masks, labels=b_labels)\n            loss = outputs.loss\n            loss = loss.mean()\n            batch_loss = loss\n        total_eval_loss += batch_loss\n    avg_val_loss = total_eval_loss / len(validation_dataloader)\n    validation_time = format_time(time.time() - t0)\n    print()\n    print(f'Validation loss: {avg_val_loss}. Validation Time: {validation_time}')\n    print()\n    # Record all statistics from this epoch.\n    training_stats.append(\n        {\n            'epoch': epoch_i + 1,\n            'Training Loss': avg_train_loss,\n            'Valid. Loss': avg_val_loss,\n            'Training Time': training_time,\n            'Validation Time': validation_time\n        }\n    )\n\nprint(f'Total training took {format_time(time.time()-total_t0)}')","metadata":{"execution":{"iopub.status.busy":"2024-01-11T09:46:28.764521Z","iopub.execute_input":"2024-01-11T09:46:28.765233Z","iopub.status.idle":"2024-01-11T10:27:26.471074Z","shell.execute_reply.started":"2024-01-11T09:46:28.765175Z","shell.execute_reply":"2024-01-11T10:27:26.470145Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stderr","text":"  0%|          | 0/1 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Beginning epoch 1 of 1\n","output_type":"stream"},{"name":"stderr","text":"  5%|▌         | 300/5581 [02:06<37:04,  2.37it/s]","output_type":"stream"},{"name":"stdout","text":"\nBatch 300 of 5581. Loss:0.5932118892669678. Time:0:02:07\nجوان گرچه دانا بود با گهر\nExample output: جوان گرچه دانا بود با گهرنگار  بران کار توانا بود از گوهر و گهر    نه گوهر بود اندر بر تو به کار    وگر نه گنج یابد تو را  \n","output_type":"stream"},{"name":"stderr","text":" 11%|█         | 600/5581 [04:13<34:57,  2.37it/s]","output_type":"stream"},{"name":"stdout","text":"\nBatch 600 of 5581. Loss:0.5389968752861023. Time:0:04:14\nببستند یارانش یکسر کمر\nExample output: ببستند یارانش یکسر کمر  دو سوی شهر به دست اندر آورد روی   سر سوی خیمه نهادند با سپاه    بزرگان با موبدان و با شهریار  \n","output_type":"stream"},{"name":"stderr","text":" 16%|█▌        | 900/5581 [06:20<32:53,  2.37it/s]","output_type":"stream"},{"name":"stdout","text":"\nBatch 900 of 5581. Loss:0.4995248019695282. Time:0:06:21\nنوندی ز گفتار کارآگهان\nExample output: نوندی ز گفتار کارآگهان  همی بود تا بر تخت عاج   ازان پس ببسته از باره ساز    ز کوشش و کوشش و رای  \n","output_type":"stream"},{"name":"stderr","text":" 22%|██▏       | 1200/5581 [08:27<30:46,  2.37it/s]","output_type":"stream"},{"name":"stdout","text":"\nBatch 1200 of 5581. Loss:0.5269736051559448. Time:0:08:27\nسرانجام اگر راه جویی بداد\nExample output: سرانجام اگر راه جویی بداد  ازان کارداران در نهان شادکام   ز گرسیوز نامه ای بر حریر    کجا کرد باید به دل ز مهر  \n","output_type":"stream"},{"name":"stderr","text":" 27%|██▋       | 1500/5581 [10:34<28:40,  2.37it/s]","output_type":"stream"},{"name":"stdout","text":"\nBatch 1500 of 5581. Loss:0.456191748380661. Time:0:10:34\nبسی آفرین کرد بر شهریار\nExample output: بسی آفرین کرد بر شهریار  که ازان جایگاه و کلاه و نگین   چو نزدیک آمد سوی ایوان و خیمه پگاه    چو در خیمهٔ پرنیان به دست  \n","output_type":"stream"},{"name":"stderr","text":" 32%|███▏      | 1800/5581 [12:41<26:30,  2.38it/s]","output_type":"stream"},{"name":"stdout","text":"\nBatch 1800 of 5581. Loss:0.46413514018058777. Time:0:12:42\nبرانگیزم از گاه کاووس را\nExample output: برانگیزم از گاه کاووس را  به فرمان یزدان بران سان که او کرد را   یکی نامه ای دارم به بهرام گرد    به فرمان او ده بر سیصدهزار  \n","output_type":"stream"},{"name":"stderr","text":" 38%|███▊      | 2100/5581 [14:48<24:28,  2.37it/s]","output_type":"stream"},{"name":"stdout","text":"\nBatch 2100 of 5581. Loss:0.39704760909080505. Time:0:14:48\nسواران ز بس رنج و اسبان ز تگ\nExample output: سواران ز بس رنج و اسبان ز تگ  به هامون چو کوهی رسید   چو آمد به هر سو فرود آورید    چو بر تخت بر جای برخاست آورید  \n","output_type":"stream"},{"name":"stderr","text":" 43%|████▎     | 2400/5581 [16:54<22:19,  2.38it/s]","output_type":"stream"},{"name":"stdout","text":"\nBatch 2400 of 5581. Loss:0.45688802003860474. Time:0:16:55\nبیامد بر تاجور سوفزای\nExample output: بیامد بر تاجور سوفزای  بفرمود تا تیغ و تیغ و سپاه   همان گنجور را تا بر پای جست    همه پیش بازارگان را بکشت  \n","output_type":"stream"},{"name":"stderr","text":" 48%|████▊     | 2700/5581 [19:01<20:13,  2.37it/s]","output_type":"stream"},{"name":"stdout","text":"\nBatch 2700 of 5581. Loss:0.40745723247528076. Time:0:19:02\nازان پس بران تیرگی بگذرم\nExample output: ازان پس بران تیرگی بگذرم  به گیتی چو دیدم همی بگسلم   همی رفت گستهم و خود کی    همی رفت با نامداران سری  \n","output_type":"stream"},{"name":"stderr","text":" 54%|█████▍    | 3000/5581 [21:08<18:06,  2.37it/s]","output_type":"stream"},{"name":"stdout","text":"\nBatch 3000 of 5581. Loss:0.44600534439086914. Time:0:21:09\nز گیتی برو بر کنند آفرین\nExample output: ز گیتی برو بر کنند آفرین  که از تو مبادا زمین آفرین   تو باید که ایمن ز رنج    که با تو که بر دل بکاهد سپنج  \n","output_type":"stream"},{"name":"stderr","text":" 59%|█████▉    | 3300/5581 [23:16<16:01,  2.37it/s]","output_type":"stream"},{"name":"stdout","text":"\nBatch 3300 of 5581. Loss:0.37840116024017334. Time:0:23:16\nگنهکار یزدانی وناسپاس\nExample output: گنهکار یزدانی وناسپاس  که یزدان فگندست در زیر پای   ز یزدان جهان آفرید از گناه    که آورد یزدان که داد و کلاه  \n","output_type":"stream"},{"name":"stderr","text":" 65%|██████▍   | 3600/5581 [25:23<13:55,  2.37it/s]","output_type":"stream"},{"name":"stdout","text":"\nBatch 3600 of 5581. Loss:0.38874903321266174. Time:0:25:23\nکنون چون زنان پیش من بسته دست\nExample output: کنون چون زنان پیش من بسته دست  بران درد و بسته ببست   بیامد برزین بر پهلوان    ز پیران و گرگین و رهام شیر  \n","output_type":"stream"},{"name":"stderr","text":" 70%|██████▉   | 3900/5581 [27:29<11:48,  2.37it/s]","output_type":"stream"},{"name":"stdout","text":"\nBatch 3900 of 5581. Loss:0.4173310399055481. Time:0:27:30\nچهارم چنین گفت شاه جهان\nExample output: چهارم چنین گفت شاه جهان پر ز راه  که از لشکر ترک جنگی مکن کینه خواه   به دل گفت شاه ای شه هوشمند    ترا داد یزدان هوشمند  \n","output_type":"stream"},{"name":"stderr","text":" 75%|███████▌  | 4200/5581 [29:36<09:41,  2.38it/s]","output_type":"stream"},{"name":"stdout","text":"\nBatch 4200 of 5581. Loss:0.4369068741798401. Time:0:29:37\nیکی داستان زد جهاندیده کی\nExample output: یکی داستان زد جهاندیده کی  که چون او نبیرهٔ کی بود کی   بدو گفت خسرو که خسرو کجاست    همی از تو دارد به آورد  \n","output_type":"stream"},{"name":"stderr","text":" 81%|████████  | 4500/5581 [31:43<07:35,  2.37it/s]","output_type":"stream"},{"name":"stdout","text":"\nBatch 4500 of 5581. Loss:0.4133557081222534. Time:0:31:44\nنگوید سخن جز همه راستی\nExample output: نگوید سخن جز همه راستی  ز دانش نگردد مرا داوری   ز گفتار ایشان برآشفت سخت    همی رفت لرزان ز راه دراز  \n","output_type":"stream"},{"name":"stderr","text":" 86%|████████▌ | 4800/5581 [33:50<05:29,  2.37it/s]","output_type":"stream"},{"name":"stdout","text":"\nBatch 4800 of 5581. Loss:0.36224663257598877. Time:0:33:51\nپرستنده گفتا چو فرمان دهی\nExample output: پرستنده گفتا چو فرمان دهی  مکن بی نیازد ترا هر کسی   به رنج تو آید همه خواسته    بماند برو کشور آراسته  \n","output_type":"stream"},{"name":"stderr","text":" 91%|█████████▏| 5100/5581 [35:57<03:23,  2.37it/s]","output_type":"stream"},{"name":"stdout","text":"\nBatch 5100 of 5581. Loss:0.3908073306083679. Time:0:35:58\nترا آشتی بهتر آید ز جنگ\nExample output: ترا آشتی بهتر آید ز جنگ  که او را کند آرزوی ننگ و جنگ   چنین گفت پیران بدانجای رفت    به سوی آن پیل رویین نشست  \n","output_type":"stream"},{"name":"stderr","text":" 97%|█████████▋| 5400/5581 [38:04<01:16,  2.38it/s]","output_type":"stream"},{"name":"stdout","text":"\nBatch 5400 of 5581. Loss:0.3219226002693176. Time:0:38:04\nمبادا ز تو تخت پردخت و گاه\nExample output: مبادا ز تو تخت پردخت و گاه  اگر داد بیند تو باشی سزاوار شاه   نگر تا ز فرمان من شاه باز    به فرجام نیک و بد بر تو گردن فراز  \n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5581/5581 [39:20<00:00,  2.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nAverage Training Loss: 0.4587778151035309. Epoch time: 0:39:21\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 621/621 [01:36<00:00,  6.40it/s]\n100%|██████████| 1/1 [40:57<00:00, 2457.68s/it]","output_type":"stream"},{"name":"stdout","text":"\nValidation loss: 0.3717266023159027. Validation Time: 0:01:37\n\nTotal training took 0:40:58\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"save_path = \"trained.pth\"  \ntorch.save(model.state_dict(), save_path)\nprint(f'Model saved at: {save_path}')","metadata":{"execution":{"iopub.status.busy":"2024-01-11T10:29:11.156530Z","iopub.execute_input":"2024-01-11T10:29:11.156954Z","iopub.status.idle":"2024-01-11T10:29:12.333449Z","shell.execute_reply.started":"2024-01-11T10:29:11.156917Z","shell.execute_reply":"2024-01-11T10:29:12.332124Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Model saved at: trained.pth\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Model Generated Poem Samples","metadata":{}},{"cell_type":"code","source":"\nsample_input = df_input[np.random.randint(0, len(df_input))]\nprint(sample_input)\nsample_input_ids = torch.tensor(tokenizer([sample_input])[\"input_ids\"])\nsample_input_ids = sample_input_ids.to(device)\n\nsample_outputs = model.module.generate(\n    input_ids=sample_input_ids,\n    do_sample=True,\n    top_k=50,\n    max_length=50,\n    top_p=0.95,\n    num_return_sequences=5\n)\nfor i, sample_output in enumerate(sample_outputs):\n    gen_sample_output = tokenizer.decode(sample_output, skip_special_tokens=False)\n    gen_sample_output = gen_sample_output.replace(\"<|startoftext|>\", \"\\n\")\n    gen_sample_output = gen_sample_output.replace(\"<s>\", \"\")\n    gen_sample_output = gen_sample_output.replace(\"</s>\", \"\")\n    gen_sample_output = gen_sample_output.replace(\"<sep>\", \"\\n\")\n\n    print(f'Example output: {gen_sample_output}')","metadata":{"execution":{"iopub.status.busy":"2024-01-11T10:44:52.394982Z","iopub.execute_input":"2024-01-11T10:44:52.395890Z","iopub.status.idle":"2024-01-11T10:44:52.958001Z","shell.execute_reply.started":"2024-01-11T10:44:52.395855Z","shell.execute_reply":"2024-01-11T10:44:52.957002Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"به شادی و انده نگردد دگر\nExample output: به شادی و انده نگردد دگر  نگونسار گردد همی خون چکاند   کسی کو بود در جهان زنده وار    به فرجام روزیش بر کشته کار  <pad><pad>\nExample output: به شادی و انده نگردد دگر  هم از گوهر شاه بی سر و هم فرو   چنین گفت رستم به نزد پسر    که ای شاه دانش پذیر و پاک دربدر  \nExample output: به شادی و انده نگردد دگر  چو روشن ببیند به خورشید سر   ز رستم فراوان بپرسید و گفت    که گر یک سوار آمدست اندر نهفت  <pad><pad><pad>\nExample output: به شادی و انده نگردد دگر  ز شاهان کند روز سر   به داد و به فر و خرد رهنمای    به فر و گهر باشد اندر سرای  <pad><pad><pad>\nExample output: به شادی و انده نگردد دگر  یکی را نشاید سپردن گذر   چنین گفت کای شاه پیروزبخت    به داد و به دانش توانا و تخت  <pad><pad><pad><pad>\n","output_type":"stream"}]},{"cell_type":"code","source":"\nsample_input = df_input[np.random.randint(0, len(df_input))]\nprint(sample_input)\nsample_input_ids = torch.tensor(tokenizer([sample_input])[\"input_ids\"])\nsample_input_ids = sample_input_ids.to(device)\n\nsample_outputs = model.module.generate(\n    input_ids=sample_input_ids,\n    do_sample=True,\n    top_k=50,\n    max_length=50,\n    top_p=0.95,\n    num_return_sequences=5\n)\nfor i, sample_output in enumerate(sample_outputs):\n    gen_sample_output = tokenizer.decode(sample_output, skip_special_tokens=False)\n    gen_sample_output = gen_sample_output.replace(\"<|startoftext|>\", \"\\n\")\n    gen_sample_output = gen_sample_output.replace(\"<s>\", \"\")\n    gen_sample_output = gen_sample_output.replace(\"</s>\", \"\")\n    gen_sample_output = gen_sample_output.replace(\"<sep>\", \"\\n\")\n\n    print(f'Example output: {gen_sample_output}')\nsample_input = df_input[np.random.randint(0, len(df_input))]\nprint(sample_input)\nsample_input_ids = torch.tensor(tokenizer([sample_input])[\"input_ids\"])\nsample_input_ids = sample_input_ids.to(device)\n\nsample_outputs = model.module.generate(\n    input_ids=sample_input_ids,\n    do_sample=True,\n    top_k=50,\n    max_length=50,\n    top_p=0.95,\n    num_return_sequences=5\n)\nfor i, sample_output in enumerate(sample_outputs):\n    gen_sample_output = tokenizer.decode(sample_output, skip_special_tokens=False)\n    gen_sample_output = gen_sample_output.replace(\"<|startoftext|>\", \"\\n\")\n    gen_sample_output = gen_sample_output.replace(\"<s>\", \"\")\n    gen_sample_output = gen_sample_output.replace(\"</s>\", \"\")\n    gen_sample_output = gen_sample_output.replace(\"<sep>\", \"\\n\")\n\n    print(f'Example output: {gen_sample_output}')","metadata":{"execution":{"iopub.status.busy":"2024-01-11T10:45:08.775708Z","iopub.execute_input":"2024-01-11T10:45:08.776495Z","iopub.status.idle":"2024-01-11T10:45:09.360170Z","shell.execute_reply.started":"2024-01-11T10:45:08.776460Z","shell.execute_reply":"2024-01-11T10:45:09.359245Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"نشسته برو شهریاری چو ماه\nExample output: نشسته برو شهریاری چو ماه  به سر بر نهاده کلاه و سپاه   چو برزد سر از کوه خورشید زرد    ز دریا همی کرد تا بود روز  <pad><pad><pad>\nExample output: نشسته برو شهریاری چو ماه  به دیهیم زرین به سر تا به ماه   همی راند تا شهر توران سپاه    یکی بندهٔ کردگار کینه خواه  <pad><pad><pad>\nExample output: نشسته برو شهریاری چو ماه  نشسته برو بر همه بندگان پیشگاه   چو بشنید ازو آن سخن رهنمای    پس آنگه ز شاه جهان کدخدای  <pad><pad>\nExample output: نشسته برو شهریاری چو ماه  که بودی به فرمان و فرمان شاه   هرآنکس که او بود بی او به جنگ    همه رنج و اندوه او بود و تنگ  \nExample output: نشسته برو شهریاری چو ماه  به نزد ز اختر یکی شاه شاه و شاه   به پیش من آر ای نامور پهلوان    که شادان بدی تا بود پهلوان  <pad><pad>\n","output_type":"stream"}]},{"cell_type":"code","source":"\nsample_input = df_input[np.random.randint(0, len(df_input))]\nprint(sample_input)\nsample_input_ids = torch.tensor(tokenizer([sample_input])[\"input_ids\"])\nsample_input_ids = sample_input_ids.to(device)\n\nsample_outputs = model.module.generate(\n    input_ids=sample_input_ids,\n    do_sample=True,\n    top_k=50,\n    max_length=50,\n    top_p=0.95,\n    num_return_sequences=5\n)\nfor i, sample_output in enumerate(sample_outputs):\n    gen_sample_output = tokenizer.decode(sample_output, skip_special_tokens=False)\n    gen_sample_output = gen_sample_output.replace(\"<|startoftext|>\", \"\\n\")\n    gen_sample_output = gen_sample_output.replace(\"<s>\", \"\")\n    gen_sample_output = gen_sample_output.replace(\"</s>\", \"\")\n    gen_sample_output = gen_sample_output.replace(\"<sep>\", \"\\n\")\n\n    print(f'Example output: {gen_sample_output}')","metadata":{"execution":{"iopub.status.busy":"2024-01-11T10:45:13.303832Z","iopub.execute_input":"2024-01-11T10:45:13.304305Z","iopub.status.idle":"2024-01-11T10:45:13.921998Z","shell.execute_reply.started":"2024-01-11T10:45:13.304265Z","shell.execute_reply":"2024-01-11T10:45:13.921088Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"بباید بریدن سر خویش پست\nExample output: بباید بریدن سر خویش پست  برستم چه باید کشیدن به دست   به گرد جهان من آن کی شوم    ازو چند هرگز نجویم بکوشم ره  \nExample output: بباید بریدن سر خویش پست  چو آمدش رای خسرو بدست   بر آنکس که خواند چنین کی قباد    نزادند یک روزگار قباد  <pad><pad><pad><pad><pad>\nExample output: بباید بریدن سر خویش پست  وزین نیز چندی دل برفشاند   چو این کرده باشی تو کردار من    چنان بدکنش بی گمان جان بدکیش من  <pad>\nExample output: بباید بریدن سر خویش پست  نجویند یک سر بریدن سر خویش پست   چو یزدان مرا دید و فرمان گزید    بدان جای بی چاره سر جان گزید  <pad><pad><pad>\nExample output: بباید بریدن سر خویش پست  اگر چاره ای هست با بیش پست   نباید کشیدن سر از تخت خویش    به روز نو سرآید بدان جایگاه  <pad><pad><pad><pad><pad>\n","output_type":"stream"}]},{"cell_type":"code","source":"\nsample_input = df_input[np.random.randint(0, len(df_input))]\nprint(sample_input)\nsample_input_ids = torch.tensor(tokenizer([sample_input])[\"input_ids\"])\nsample_input_ids = sample_input_ids.to(device)\n\nsample_outputs = model.module.generate(\n    input_ids=sample_input_ids,\n    do_sample=True,\n    top_k=50,\n    max_length=50,\n    top_p=0.95,\n    num_return_sequences=5\n)\nfor i, sample_output in enumerate(sample_outputs):\n    gen_sample_output = tokenizer.decode(sample_output, skip_special_tokens=False)\n    gen_sample_output = gen_sample_output.replace(\"<|startoftext|>\", \"\\n\")\n    gen_sample_output = gen_sample_output.replace(\"<s>\", \"\")\n    gen_sample_output = gen_sample_output.replace(\"</s>\", \"\")\n    gen_sample_output = gen_sample_output.replace(\"<sep>\", \"\\n\")\n\n    print(f'Example output: {gen_sample_output}')","metadata":{"execution":{"iopub.status.busy":"2024-01-11T10:45:16.056922Z","iopub.execute_input":"2024-01-11T10:45:16.057666Z","iopub.status.idle":"2024-01-11T10:45:16.605929Z","shell.execute_reply.started":"2024-01-11T10:45:16.057631Z","shell.execute_reply":"2024-01-11T10:45:16.604895Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"به نزد بزرگان ایرانیان\nExample output: به نزد بزرگان ایرانیان  همی رفت باید به پیش پدر   چو بهرام یل رابخواند درست    که از درد و تیمار او برفشاند نخست  \nExample output: به نزد بزرگان ایرانیان  فرستاد یکسر دوال کمر   چنان چون به هر جای فرمان گزید    همه لشکر آراسته برگزید  <pad><pad><pad><pad><pad><pad>\nExample output: به نزد بزرگان ایرانیان  به تخت بزرگی به پای میان   به دست چپش بر یکی پهلوان    همی تاختندی بدین پهلوان  <pad><pad><pad><pad><pad>\nExample output: به نزد بزرگان ایرانیان  ز شاه آفریدی به زین   به مردی ز شاه جهان آگهی    کجا بود در جهان آگهی  <pad><pad><pad><pad><pad>\nExample output: به نزد بزرگان ایرانیان  ببستند با او میان   چو او بر در دژ دژ رسید آگهی    پس او را سر آمد سپه دهی  <pad><pad>\n","output_type":"stream"}]},{"cell_type":"code","source":"\nsample_input = df_input[np.random.randint(0, len(df_input))]\nprint(sample_input)\nsample_input_ids = torch.tensor(tokenizer([sample_input])[\"input_ids\"])\nsample_input_ids = sample_input_ids.to(device)\n\nsample_outputs = model.module.generate(\n    input_ids=sample_input_ids,\n    do_sample=True,\n    top_k=50,\n    max_length=50,\n    top_p=0.95,\n    num_return_sequences=5\n)\nfor i, sample_output in enumerate(sample_outputs):\n    gen_sample_output = tokenizer.decode(sample_output, skip_special_tokens=False)\n    gen_sample_output = gen_sample_output.replace(\"<|startoftext|>\", \"\\n\")\n    gen_sample_output = gen_sample_output.replace(\"<s>\", \"\")\n    gen_sample_output = gen_sample_output.replace(\"</s>\", \"\")\n    gen_sample_output = gen_sample_output.replace(\"<sep>\", \"\\n\")\n\n    print(f'Example output: {gen_sample_output}')","metadata":{"execution":{"iopub.status.busy":"2024-01-11T10:45:17.295885Z","iopub.execute_input":"2024-01-11T10:45:17.296259Z","iopub.status.idle":"2024-01-11T10:45:17.846384Z","shell.execute_reply.started":"2024-01-11T10:45:17.296221Z","shell.execute_reply":"2024-01-11T10:45:17.845412Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"کنون روز دادست بیداد شد\nExample output: کنون روز دادست بیداد شد  جهان را همه روز کامروز شد   تو با من چنان چون گمانی مکوش    مرا رنج و سختیست و چندین خروش  \nExample output: کنون روز دادست بیداد شد  ز هر گونه ای کرد کردار یاد   همان روز گشتاسپ اندر گذشت    بدیب و به خون و به دل کینه گشت  <pad><pad>\nExample output: کنون روز دادست بیداد شد  که از باد روز نبرد   چنین است گیتی پر آواز من    بدین گونه بر کام تو یافتم  <pad><pad><pad><pad><pad><pad><pad>\nExample output: کنون روز دادست بیداد شد  جهان زیر فر چرخ و آهنگ شد   تو از من به من چون توانگر شوی    به فرمان شاه جهان بر شوی  <pad><pad>\nExample output: کنون روز دادست بیداد شد  کزین بد چه روز بیداد شد   چه در شهر مازندران    که هر ماه رخ شاهوار ماه  <pad><pad><pad><pad><pad><pad>\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}